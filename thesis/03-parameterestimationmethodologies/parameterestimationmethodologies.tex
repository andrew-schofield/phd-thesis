\svnid{$Id$}
\chapter{Parameter Estimation Methodologies}
\label{chap:paramest}

\section{The Challenge of Parameter Estimation}
Parameter estimation is a technique that attempts to estimate the values of a set of parameters based on measured data. This involves creating an estimator - an algorithm that generates new parameter estimates - and some form of calculation to assess how good the estimate is. The model being discussed in this thesis has a large number of unknown values which need to be estimated and it was not clear what values many of these parameters should take, or even if the initial values used are close to the ``true'' values. Given the nature of biology, point estimates for parameters are not going to be representative of the true state of that parameter, thus the output of the parameter estimation needed to be a probability distribution that reflected the likelihood of the parameter taking on a particular value. Another requirement of the estimator is that it needs to be able to select new estimates based on prior probability distributions, since some information about certain parameters is known beforehand, and this knowledge will improve as more datasets are analysed.

These requirements lend themselves well to Monte-Carlo type methods, which produce output which can be represented as a probability distribution and can be selected from a prior probability distribution.

\section{Methods and Algorithms Used}
Below are the various different methods for parameter estimation that were investigated during the course of this work. All of these methods involve a sampling system that attempts to select parameter values which minimise the difference between the simulation result and the experimental data by using a difference function. The main difference between the parameter estimation methodologies laid out below are the ways in which they generate new parameter values, either from a probability distribution or based on the previous value, and the way this is applied and tested against the experimental data. The aim of the parameter estimation methodology was to achieve an output from the difference function that was as close to zero (perfect fit/no difference) as possible. The value produced by the difference function is hereby referred to as the \textit{Badness of Fit} or \textit{BOF} value, due to larger values representing worse fits to the experimental data.

\subsection{Difference Functions}
Two different difference functions were used to calculate the \textit{BOF} value. One was the sum of the Least Squares Differences between the measured components in the experimental data and the simulation result. This was used in early parameter estimation runs.

\begin{equation}
BOF = \sum^{n}_{j=1}\left(\sqrt{\sum^{m}_{i=1} (\Delta x_{ij})^2}\right)
\end{equation}
Where $j$ is the current parameter, $n$ the number of parameters, $i$ the current data point, $m$ the total number of data points and $(\Delta x_{ij})^2$ the square of the residuals between the experimental and solved data for parameter $j$ at data point $i$. Therefore the \textit{BOF} value becomes ``the sum of the squares of the residuals for each data point for each parameter.''

The second method for calculating the \textit{BOF} was a log-likelihood calculation. This method calculates the likelihood of the solved data falling under a normal distribution which has the experimental data as its $\mu$, and a preset error value as its $\sigma$. The method was used as it allowed tuning of the $\sigma$ value, which provided more control over how new parameter sets were accepted. The lower the value of $\sigma$, the closer the solved data needs to be to the experimental data to achieve a low \textit{BOF} value.

\begin{equation}
BOF = \sum^{n}_{j=1}\left(\sum^{m}_{i=1}\ln(norm(x_1,\mu_1,\sigma)) - \ln(norm(x_2,\mu_2,\sigma))\right)
\end{equation}
Where $j$ is the current parameter, $n$ is the number of parameters, $i$ is the current data point, $m$ is the total number of data points, $norm$ is a function that accepts 3 arguments ($x, \mu, \sigma$) and returns the probability associated with $x$ in a normal distribution with parameters $\mu$ and $\sigma$, $x_1$ and $\mu_1$ are zero, $x_2$ is the solved value at data point $i$ for parameter $j$, $\mu_2$ is the experimental value at data point $i$ for parameter $j$ and $\sigma$ is the preset standard deviation.

$x_1$ and $\mu_1$ are set to zero because this likelihood calculation can also be used on the time axis in addition to the value axis. Since there is no time uncertainty in the data, this can be ignored by setting these values to zero. The value for $\sigma$ is determined such that the first few iterations of parameter estimation generate very high \textit{BOF} values (with the knowledge that this will always decrease) subject to the computational limits imposed by floating point precision (values of $\sigma$ which are too low generate nonsensical \textit{BOF} values).

In most cases the components used for calculating the likelihood are Oxygen and Nitric Oxide, as these were the primary measured chemicals.
\subsection{Monte Carlo Methods}
A brief section is included here to describe Monte Carlo Methods, as the techniques used for parameter estimation all use this method as their estimator in some form.
``Monte Carlo Methods'' is a generalised term to describe a stochastic technique that makes use of random numbers to examine a problem in conjunction with probability statistics. Monte Carlo Methods allow modelling of complex systems without having to exhaustively search every possible outcome. Large systems are sampled in random configurations and that data applied in such a way that it can be used to describe the system as a whole. ``Monte Carlo techniques are often the only practical way to evaluate difficult integrals or to sample random variables governed by complicated probability density functions''\cite{Nakamura2010}. The generalized form of Monte Carlo Methods involves generating a set of random numbers which conform to a particular probability distribution (depending on the problem) and observing the fraction of those numbers which fulfil a particular set of properties (when transformed by some algorithm). The larger the number of random numbers generated, the more useful the output.

As a simple example, the value of $\pi$ can be estimated using a Monte Carlo method. With the prior knowledge that $\frac{\pi}{4}$ is equal to the area of a circle with diameter $d$, divided by the area of a square with sides $d$ (see appendix), the value of $\pi$ can be estimated simply by generating uniformly distributed random points on the square and the counting the number that also lie inside the circle. The value of $\pi$ is $4\times\frac{\mathrm{number~of~points~in~circle}}{\mathrm{total~number~of~points}}$. The estimate for the value of $\pi$ improves with the number of points (and in this case converges slowly on the true value).

\subsubsection{Markov Chain Monte Carlo}
Markov Chain Monte Carlo algorithms use Monte Carlo Methods to create a Markov chain whose stationary distribution is the desired (posterior) distribution. Markov Chains are sequences of states where state $x_{t+1}$ depends only on state $x_t$. Given suitably large numbers of states, the chain will forget the initial state $(x_0)$ and the resulting chain will represent the posterior (stationary) distribution. This distribution won't depend on either $t$ or $x_0$\cite{Gilks1996}. The states in the chain prior to arriving at the stationary distribution are known as ``burn in'' and are discarded, and the stationary distribution is the usable output which can then be converted into a more convenient format to use as a posterior probability distribution.

\subsection{Simulated Annealing}
\begin{figure}[htbp]
\small
\begin{verbatim}
c1 = c0;
c2 = mutate(c1);
i = 0;
while i < i_max
  if fitness(c1) > fitness(c2)
    c2 = mutate(c1)
  else
    c1 = mutate(c2)
  i = i + 1
if fitness(c1) > fitness(c2)
  return c1
else
  return c2
\end{verbatim}
\caption[Pseudo-code showing how a rejection sampler works.]{{\bf Pseudo-code showing how a rejection sampler works.}
\label{fig:sa_code}}
\end{figure}
\noindent This technique was described independently by \citet{Kirkpatrick1983} and \citet{Cerny1985}. The name comes from the metallurgical annealing process whereby large crystals are formed while a material is slowly cooled. The slow cooling increases the probability of individual crystals obtaining lower energy states than the initial. As the material cools the ``distance'' each crystal can move along the energy landscape decreases. In a simulated annealing algorithm, ``distance'' is controlled by a perturbation kernel which takes the ``temperature'' as input to change how much each parameter can be perturbed.
Simulated annealing ``consists of a discrete-time inhomogeneous Markov chain''\cite{Bertsimas1993} whereby the previous state is modified with a perturbation kernel (the neighbouring states) and then accepted or rejected using a transition probability which depends on the current temperature and the energies of the previous and current states. The advantage of this scheme is that areas of local minima have a lesser effect on the outcome of the Markov chain as the high initial temperature allows for the chain to ``jump'' out of these minima.

Simulated annealing is an extension of a simple rejection sampler \cite{Marjoram2003}. Figure~\ref{fig:sa_code} contains some simple pseudo-code which shows how the basic rejection sampler algorithm works. This will provide a ``best'' parameter set, but there is no information about the possible spread of values in the parameter set. Given that it is unlikely that a single point-value parameter-set solution exists that will accurately describe the system it is necessary to produce a spread of results that will adequately describe the system instead. To this end a modified version of simulated annealing was used integrated with aspects of a simple genetic algorithm. In the genetic algorithm paradigm a synthetic ``chromosome'' is created which contains ``genes'' representing the parameters in the simulation. These include the rate constants, concentrations of various components and initial concentrations of substrates and products. This chromosome is then copied and perturbed several times (depending on the eventual population size required), with the size of the perturbation being dependent on the current annealing temperature. For instance the highest temperature could indicate that the individual parameters can be perturbed by up to $\pm 10\%$, and as the temperature decreases the perturbation percentage has a concomitant decrease. An example annealing temperature schedule is shown in Figure \ref{fig:temperature}.
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[height=10cm]{03-parameterestimationmethodologies/data/temperature.png}
	\caption[Example simulated annealing temperature schedule]{{\bf Example simulated annealing temperature schedule.} This represents how the annealing temperature changes as the annealing process progresses. Temperature is denoted by the y-axis value, and simulation progress by the x-axis value. This is a heavily compressed time-frame, as in reality the system would normally process 10,000 sets of parameters at each annealing temperature.
	\label{fig:temperature}}
	\end{center}
\end{figure}
The annealing temperature is programmed to decrease after a defined number of iterations such that the magnitude of individual mutations becomes smaller as the simulation progresses. This should have the effect of honing in on a set of parameters with a better likelihood.
Once the chromosome population has been created, 2 are selected at random and their likelihoods evaluated, in this case by the Least Squares Difference method. The chromosome with the lowest likelihood is discarded and the other is cloned and perturbed. The two chromosomes are then added back to the chromosome population. The genetic algorithm is used to improve the parameters sets by perturbing genes (individual parameters) from fit chromosomes (complete parameter sets), re-running the simulation and discarding unfit parameter sets. An unfit parameter set is defined as one with an likelihood lower than the highest so far. This technique involves having two chromosomes selected at any one time. The parameters from each chromosome are simulated and the least fit one is discarded. At this point the contents from the fitter chromosome are cloned and perturbed, and the cycle is repeated. Figure~\ref{fig:sa_spread} shows this diagrammatically. After many cycles (upwards of 1000) the chromosome pool should only contain the best fitting parameter sets to the experimental data. The spread of the parameters can be used to infer the sensitivity of the simulation to changes in parameter values in a similar way to that described by \citet{Toni2009}.
\begin{figure}[tbp]
\begin{center}
\includegraphics[height=10cm]{./03-parameterestimationmethodologies/data/sa_spread.pdf}
% sa_spread.pdf: 479x520 pixel, 72dpi, 16.90x18.34 cm, bb=0 0 479 520
\end{center}
\caption[{Schematic diagram showing the technique used to generate a spread of parameters using a synthetic chromosome.}]{{\bf Schematic diagram showing the technique used to generate a spread of parameters using a synthetic chromosome.} The parameters are loaded as genes on the chromosome which are then perturbed, 2 chosen and the fittest kept and perturbed. Each time a chromosome is perturbed it is reintroduced into the chromosome pool, and the next 2 chromosomes are chosen at random.
\label{fig:sa_spread}}
\end{figure}
%\textbf{Advantages}
%\begin{itemize}
%	\item Easy to implement.
%	\item Can avoid local energy minima by jumping when the temperature is high.
%	\item Energy need only be calculated for 2 samples at any one time.
%	\item High optimisation performance on some problems.
%\end{itemize}
%\textbf{Disadvantages}
%\begin{itemize}
%	\item Slow performance on certain problems.
%	\item Cannot produce probability distributions.
%\end{itemize}
%\textbf{Reason(s) for rejection}
%\begin{itemize}
%	\item Seemed to be incapable during testing of settling on solutions to Nitric Oxide reduction datasets.
%	\item No probability distributions which were eventually required to incorporate data from previous datasets.
%\end{itemize}

This algorithm was rejected as unsuitable as it seemed to be incapable of settling on solutions to the Nitric Oxide reduction datasets during testing. Additionally this technique does not produce probability distributions which were eventually required to incorporate data from previous datasets.

%The algorithm is flexible in that it includes a gene mask in the chromosome which allows any number of genes to be switched on or off. In this way it is possible to fix the values of certain parameters, and prevent them from being perturbed at each iteration. This is useful during the parameter search process, as a limited dataset can be used, which only describes a limited subset of the overall model, and any parameters which are unrelated to that dataset can be removed. Practically this means it is possible to focus on one particular aspect of the model, such as oxygen utilisation, set all the parameters which are irrelevant to zero, and prevent them from being included in the genetic algorithm. During this process, the parameters related to oxygen utilisation can be improved, and once a sufficient set of value is obtained, these can then be fixed when a different dataset is used, such as \cNO \space reduction.

\subsection{Approximate Bayesian Computation by Sequential Monte Carlo}
``Approximate Bayesian Computation methods have been conceived with the aim of inferring posterior distributions where likelihood functions are computationally intractable or too costly to evaluate. They exploit the computational efficiency of modern simulation techniques by replacing calculation of the likelihood with a comparison between the observed data and simulated data''\cite{Toni2009}.

To incrementally improve the parameter sets, a version of Bayesian inference is used in conjunction with a standard Monte Carlo method in a system called Approximate Bayesian Computation by Sequential Monte Carlo (ABCSMC)\nomenclature{ABCSMC}{Approximate Bayesian Computation by Sequential Monte Carlo} as described by \citet{Toni2009}. An implementation of algorithm (S) was used from that paper.

\textbf{\textit{Bayesian Inference}} - This is a statistical method for inferring the probability of a hypothesis based on available evidence. As more evidence is accumulated, the inference is updated and the probability of the hypothesis being true is changed. Given enough evidence, the probability of the hypothesis being true should either be very high or very low causing you to either accept or reject the hypothesis. Bayesian inference relies on having a prior probability (or probability distribution) for the hypothesis, and this can inevitably introduce a level of bias into the inference.
Bayesian inference can be described thus:
\begin{equation}
P(H\mid E) = \dfrac{P(E\mid H)}{P(E)}\cdot P(H)
\label{eq:bayes}
\end{equation}
\begin{itemize}
	\item $P(H\mid E)$ is the posterior distribution of $H$ given $E$.
	\item $P(H)$ is the prior distribution
	\item $\dfrac{P(E\mid H)}{P(E)}$ is the impact of $E$ on the degree of belief in $H$.
\end{itemize}

A simplistic example of using Bayesian inference to alter a hypothesis could happen in the case of having two jars of sweets. Jar 1 has 15 strawberry sweets and 25 raspberry sweets. Jar 2 has 20 of each. Supposing a third party selects 1 sweet at random from 1 of the jars. They select a strawberry sweet, what is the probability that it came from Jar 1? From the point of view of our third party both jars are identical, therefore $P(H_1) = P(H_2)$ and the total probability must equal 1, so the prior probability of each jar is 0.5. The observation, $E$ is of a strawberry sweet, which we can then use to calculate the likelihood of it being from each jar individually by $P(E\mid H_1) = 15/40 = 0.375$ and $P(E\mid H_2) = 20/40 = 0.5$.
The Generalised Bayes formula can then be used to work out the probability of the strawberry sweet being from jar 1, that is $P(H_1\mid E)$.\\
\begin{align}
\nonumber
P(H_1\mid E) & = \dfrac{P(E\mid H_1)P(H_1)}{P(E\mid H_1)P(H_1) + P(E\mid H_2)P(H_2)}\\
\nonumber \\
\nonumber
& = \dfrac{0.375 \times 0.5}{0.375 \times 0.5 + 0.5 \times 0.5}\\
\nonumber \\
& = 0.429
\end{align}

Before the observation of the sweet, the probability of the third party taking from jar 1 was the prior probability of $0.5$. After the observation this probability must be revised to 0.429.

Bayesian inference is widely used in computational analysis for artificial intelligence and email spam identification. It is also used in the field of population genetics and phylogenetics\cite{Ronquist2011}.

\textbf{\textit{Approximate Bayesian Computation}} - This is an adaptation of Bayesian inference which allows approximately the same inferences to be made, with considerably less computation. It operates on representations of the datasets rather than the datasets themselves. Common examples are population mean and variance. This is useful for large complex datasets where the probability of a simulation of the dataset matching the original is very small (unacceptably so), in this case a representation of the datasets can be used, and the difference calculated. If the difference is less than a pre-defined acceptance threshold, then the simulated dataset is accepted. \nomenclature{ABC}{Approximate Bayesian computation}ABC originally came from the fields of population and evolutionary genetics\cite{Beaumont2002}, but is now being applied to complex and stochastic dynamical systems\cite{Sisson2007,Toni2009,Beaumont2010}.

ABC differs from standard Bayesian inference shown in Equation \ref{eq:bayes} in that the likelihood term does not need to be calculated. Instead the difference between the summary statistics of the observed data and the simulated data is used. The simulated data is considered a true sample from the posterior distribution if the difference in summary statistics is less than a predefined acceptance threshold.
The most basic ABC methods takes the following form:\\
$\theta$ is a parameter vector to be estimated, $\pi(\theta)$ is the prior probability distribution, and $x$ is the observed data. The posterior distribution is $\pi(\theta \mid x) \propto f(x \mid \theta)\cdot \pi(\theta)$.
\begin{enumerate}
	\item Create a candidate parameter vector $\theta^*$ from the prior distribution $\pi(\theta)$.
	\item Simulate dataset $x^*$ using the model and parameter vector $\theta^*$.
	\item Compare $x^*$ with $x$ using a distance function $d$ and an acceptance criteria $\epsilon$. If $d(x,x^*)\leq \epsilon$, accept $\theta$.
\end{enumerate}

Given a low enough value for $\epsilon$, the output distribution should approximate the true posterior distribution if sampled a large enough number of times.

\textbf{\textit{Sequential Monte Carlo}} - This is a method of particle filtering whereby a large set of samples ($N$) are drawn from the prior distribution, and for each sample, the probability is calculated. Weights for each particle are assigned based on the probabilities, and these affect how likely a particle is to be selected in subsequent rounds of selection. At the end of each round, the posterior distribution of the $N$ particles becomes the prior distribution for the next round.

\textbf{\textit{Approximate Bayesian Computation by Sequential Monte Carlo}} - This combines the previous two methods by drawing a large number of particles from the prior distribution using Bayesian inference. The prior distribution is discrete in the scheme used here, so a perturbation kernel based on a Laplacian or Gaussian distribution is used on each sample to provide small deviations to better approximate a continuous prior distribution. Each sample is simulated and only accepted if it exceeds the acceptance threshold. This is calculated based on the \nomenclature{LSD}{Least-Squares Difference}least-squares difference (LSD) between the simulated data and the original dataset. If a sample is rejected, a new one is drawn from the prior distribution and SMC then continues as described above. The weights of the accepted samples are calculated based on the probabilities of being selected from the prior and the samples go on to form the posterior distribution. For each subsequent round, the mean LSD of the posterior distribution from the previous round is used as the acceptance threshold. This ensures that each round results in better fitting parameter sets. The cycle is then repeated until a pre-defined cycle limit is reached \cite{Toni2009}.

A significant advantage of this technique is that it is readily parallelisable, as each particle in the SMC\nomenclature{SMC}{Sequential Monte Carlo} process is independent, thus can be simulated in parallel. A parallel version of this algorithm was implemented in the JAVA programming language which resulted in significant speed-ups when multiple processing threads can be used. The threading manager means that the algorithm is theoretically most efficient (in terms of computational time) when the number of particles is an exact multiple of the number of processing threads. In practice however this is negated by the fact that some particles require multiple samples due to them not meeting the acceptance criteria.

%\textbf{Advantages}
%\begin{itemize}
%	\item Paralellisable. Leading to improved performance on multiprocessor systems.
%\end{itemize}
%\textbf{Disadvantages}
%\begin{itemize}
%	\item Requires a suitable distance function or summary statistic.
%\end{itemize}
%\textbf{Reason(s) for rejection}
%\begin{itemize}
%	\item Seemed incapable of settling on sensible posterior distributions with some datasets. I suspect the distance function used was causing a conflict which meant the algorithm was accepting bad parameter sets and rejecting good ones.
%\end{itemize}

This algorithm was rejected on the grounds that it seemed incapable of settling on sensible posterior distributions with some datasets. It is suspected that the distance function used was causing a conflict which meant the algorithm was accepting bad parameter sets and rejecting good ones. The requirement of a suitable distance function or summary statistic is a major disadvantage of this algorithm.

\subsection{Metropolis Hastings Monte Carlo}
The Metropolis-Hastings algorithm is a Markov Chain Monte Carlo method to retrieve sequences of random samples from a probability distribution which cannot be sampled directly (or would be very difficult) such as when no probability distribution function exists. Such a case exists when the proposal distribution is a discrete rather than a continuous distribution. The algorithm was originally developed by \citet{Metropolis1953} for generating samples from the Boltzmann distribution. It was later extended to a more general form for any distribution by \citet{Hastings1970}.

This algorithm is much simpler than the previously described ABCSMC and takes the following form\cite{Gilks1996}:

\singlespacing %necessary to produce nice code
\begin{alltt}
Initialise \(X_0\); set \(t=0\);
Repeat \{
  Sample a point \(Y\) from \(q(.|X\sb{t})\)
  Sample a Uniform(0,1) random variable \(U\)
  If \(U \leq \alpha(X\sb{t},Y)\) set \(X\sb{t+1}=Y\)
    otherwise set \(X\sb{t+1}=X_t\)
  Increment t
\}
\end{alltt}
\doublespacing %remember to un-single space!

For the parameter estimation problem being solved, in the algorithm above, $X_0$ is the initial set of parameters, $t$ is the iteration number, $q(.\mid X_t)$ is the \textit{proposal distribution} which is the prior probability distribution and $\alpha(X_t,Y)$ is the acceptance probability. The acceptance probability is calculated from the likelihood of $Y$ being from the prior probability distribution and the likelihood of the solved data being from a normal distribution surrounding the experimental data. Mathematically it is described thus:
\begin{equation*}
\alpha(X_t,Y) = \frac{q(Y_t\mid X)}{q(Y_{t-1}\mid X)}\cdot\e{}^{BOF(Y_{t-1}) - BOF(Y_t)}
\end{equation*}
This process can be described as below:
\begin{enumerate}
\item Create a set of starting parameter values, calculate the probability of them from the prior distribution, and their likelihood of fitting the experimental data.
\item Sample new parameter values from the prior probability distribution, calculate probabilities and likelihoods.
\item Calculate acceptance based on the above values.
\item If acceptance is greater than a random number generated in the range 0-1 then save the new parameters. (Acceptance will be greater than 1 if the parameters are more likely \textit{and} the results fit better). This allows slightly less probable, and less likely results to be accepted
\item Otherwise save the old parameters.
\item Go Back to step 2
\item After enough repeats the saved results will resemble the posterior probability distribution.
\end{enumerate}
This was the algorithm selected to perform the parameter estimation as it was simple to implement, generated the required probability distributions and didn't require complex functions to determine acceptance. Its only major disadvantage was that it was an unparallel algorithm, thus it was quite slow as each iteration depended on the results of the previous.

\section{Validating the Parameter Estimation Algorithm}

The Metropolis Hastings Monte Carlo algorithm was validated by using a much simpler ODE system than is required by the respiratory model, to make sure it was capable of parameter estimation and producing the necessary output for creating probability distributions. The simpler ODE system selected was the Lotka-Volterra model as this can be tested much more quickly by virtue of having far fewer parameters to estimate (4 as opposed to \gt 20) and has only 2 differential equations to solve. The Lotka-Volterra model describes a simple predator-prey relationship\cite{Lotka1925,Volterra1931} and only requires two first-order, non-linear differential equations, which are shown in equation \ref{eq:lv}.
\begin{eqnarray}
\frac{dx}{dt} = x (\alpha - \beta y)\nonumber \\
\frac{dy}{dt} = -y (\gamma - \delta x)
\label{eq:lv}
\end{eqnarray}

The algorithm was validated by generating a dataset using the Lotka-Volterra equations with a known set of parameters. The dataset was generated using the same ODE solving equations described in Chapter \ref{chap:model} for 1000 generations. The Metropolis Hastings algorithm was given uniform distributions to sample from for each of the priors (this means that there is more-or-less no sampling distribution), and the acceptance criteria for each iteration were based upon the log-likelihood (described previously) values between the input dataset and the solved output from the new parameters. The algorithm was allowed to continue for 10000 iterations, generating 4 Markov-Chains of length 10000. During this time the algorithm will be altering the 4 input parameters trying to improve them based on the log-likelihood values produced. The resulting Markov Chains represented the stationary or posterior distributions of the 4 parameters and were very close to the true values from the input data.

Given the simplicity of this system, a particularly bad set of initial parameter estimates (i.e. $x_0$ as described previously) were given to exaggerate the burn in period, and to show that given a long enough time the algorithm will eventually settle on ``correct'' values. This validation step also informs the likely values for two tuning variables in the algorithm; the \textit{acceptance} - how stringent the algorithm is on accepting new parameter sets, which is the $\sigma$ value of the log-likelihood calculation, and the \textit{internal sigma} value - this describes the magnitude of parameter perturbation at each iteration.
The graphical results of the Lotka-Volterra validation are shown in Figures \ref{fig:simulation} and \ref{fig:parameters}. Figure \ref{fig:simulation} shows the generated validation data compared to the solved output from the parameters at the end of the Markov Chains. Figure \ref{fig:parameters} shows a plot of the 4 Markov Chains generated by the MHMC algorithm. The initial 3000 generations are the burn-in and represent the region of the Markov Chain where the algorithm is trying to converge on the stationary distribution. The region beyond 3000 generations \textit{is} the stationary distribution and this can be used to calculate a posterior probability distribution (the sampling distribution was flat).

The results of this validation show that the algorithm is capable of parameter estimation and can produce output that is compatible with generating probability distributions. The comparison of the Lotka-Volterra data shows that the algorithm could have been run for more iterations as the solved output doesn't quite match the generated input data. The Markov Chains show that the algorithm has stabilised on values very close to the ``true'' values of the input data.

\begin{figure}[tbp]
 \centering
 \includegraphics[width=13cm, trim=50px 35px 125px 30px, clip=true]{./03-parameterestimationmethodologies/data/LV_data.pdf}
 % Simulation.png: 800x600 pixel, 72dpi, 28.22x21.17 cm, bb=0 0 800 600
 \caption[{Simulation results of the Lotka-Volterra validation run.}]{{\bf Simulation results of the Lotka-Volterra validation run.}
 \label{fig:simulation}}
\end{figure}

\begin{figure}[tbp]
 \centering
 \includegraphics[width=13cm, trim=50px 35px 125px 30px, clip=true]{./03-parameterestimationmethodologies/data/LV_MCMC.pdf}
 % Simulation.png: 800x600 pixel, 72dpi, 28.22x21.17 cm, bb=0 0 800 600
 \caption[{MHMC results of the Lotka-Volterra validation run.}]{{\bf MHMC results of the Lotka-Volterra validation run.} Note the initial burn-in period followed by the distribution trajectory.
 \label{fig:parameters}}
\end{figure}
\afterpage{\clearpage}

\section{An Integrated Parameter Estimation Scheme Combining MHMC With Bayesian Inference}
In order to correctly parametrise the complex model being investigated it would have been impossible to use a MHMC-based parameter estimation algorithm alone, since it would be difficult to generate dataset which could provide enough information to estimate all the parameters at once. This being the case the parameter estimation algorithm was combined with a Bayesian approach to generating the proposal/prior distributions. Each run of MHMC will generate a Markov-Chain which can be used to calculate a posterior probability distribution for each parameter. These posterior probability distributions could be used to inform a subsequent round of parameter estimation in a Bayesian manner by using these distributions as the proposal/prior distributions for that subsequent round. This approach relies on the overall problem being able to be broken down into smaller problems which add increasing amounts of information. Thus the initial round of parameter estimation may only be estimating a small percentage of the total number of parameters, \textit{but} the posterior probability distributions generated can be used as informed prior probability distributions for a second round of parameter estimation which include more parameters. The entire scheme is represented visually in Figure \ref{fig:intscheme}.

\begin{sidewaysfigure}[tbp]
 \centering
 \includegraphics[width=20.4cm]{./03-parameterestimationmethodologies/data/paramest.pdf}
 \caption[{The Integrated Bayesian Parameter Estimation Scheme}]{{\bf The Integrated Bayesian Parameter Estimation Scheme.} This figure shows a flow-chart representing the major steps in the scheme. The initial probability distributions may be uniform, depending on how much knowledge is available \textit{a prioi}. The experimental data is compared to the solved data in the purple box. After sufficient iterations of parameter estimation, the Markov Chains are used to produce posterior probability distributions.
 \label{fig:intscheme}}
\end{sidewaysfigure}

\section{Implementing the Integrated Scheme}
As described previously, one dataset alone is not sufficient to provide enough information to be able to parametrise the model. In order to correctly perform Bayesian informed parameter estimation, the mathematical model needed to be separated into simpler units which could more easily be described by specific sets of experimental data. These simpler units would each provide information on a subset of the total number of parameters in the model.

The model was separated such that the simplest parts could be parametrised first. In this case, the simplest section of the model was those equations which describe oxygen respiration. This only requires one enzyme (although it does still require the electron transport chain). This section of the model also has the simplest experimental dataset.

These simplified datasets are parametrised, with each dataset analysed at least 10 times, generating at least 10 Markov Chains per parameter. These repeats are performed to provide statistical significance by reducing bias that could be introduced by using only 1 Markov Chain.

After this section of the model has been parametrised, posterior probability distributions are calculated, new sections are added and the process (shown in Figure \ref{fig:intscheme}) repeated, until all of the parameters have been estimated.

At this point the output probability distributions should be representative of the true parameter values for all the components in the model, subject to certain caveats such as the fact that parts of the model are simplified. These probability distributions should be accurate enough that they can be used to predict the behaviour of the biological system \textit{in vivo}.

%Experimental data was gathered for the first dataset, and this is used as a training set, with almost flat priors based on preliminary experimental data. The data was pre-processed and normalised if necessary and then presented to the Bayesian Parameter Estimation system. The MHMC algorithm samples from the prior distribution and then uses those samples as parameters to solve the model. It aims to improve the calculated likelihood and is ordinarily run for at least 10,000 iterations to give the system time to settle on the fittest parameters. The system is run on the same dataset 10 times to generate statistically significant results. The eventual output of the MHMC runs are posterior probability distributions for each of the parameters in the model. In accordance with Bayesian inference these are then used as prior distributions.

%At this point the next section of the model to be parametrised is decided, the appropriate experimental data identified and obtained, and the process is repeated until the entire model has been populated. The final result should be a set of reasonably narrow probability distributions for each of the parameters which describe the system accurately enough to correctly predict the behaviour of the system \textit{in vivo}.